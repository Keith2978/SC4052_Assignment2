{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Code Implementation\n",
        "\n",
        "In this section, we implement the **PageRank algorithm**. The algorithm computes the PageRank scores based on the transition matrix derived from the adjacency matrix of the graph. We also incorporate the **teleportation factor** to ensure stability in the algorithm, especially for handling dangling nodes and ensuring convergence.\n",
        "\n",
        "The code below:\n",
        "- Normalizes the adjacency matrix to create the **transition matrix**.\n",
        "- Uses the **power iteration method** to iteratively compute the PageRank scores until convergence.\n",
        "- Runs the algorithm for different **damping factors** to analyze the effect of the teleportation probability on the stability and convergence of the rankings.\n"
      ],
      "metadata": {
        "id": "yLbthbt9iAw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the transition matrix M (adjacency matrix representing forward links)\n",
        "M = np.array([\n",
        "    [0, 0, 1],\n",
        "    [0.5, 0, 0],\n",
        "    [0.5, 1, 0]  ])\n",
        "\n",
        "# Number of pages (nodes)\n",
        "n = M.shape[0]\n",
        "\n",
        "# Define the teleportation probability (p)\n",
        "p = 0.85\n",
        "\n",
        "# Initialize the PageRank vector\n",
        "pagerank = np.ones(n) / n\n",
        "\n",
        "# Create the teleportation vector q (assuming each page has a different probability of being visited)\n",
        "# q can represent any custom distribution based on how likely each page is to be visited.\n",
        "q = np.array([0.333333, 0.333333, 0.333333])\n",
        "\n",
        "# Number of iterations to run\n",
        "iterations = 10\n",
        "\n",
        "# Calculate the PageRank iteratively\n",
        "for i in range(iterations):\n",
        "    # Compute the PageRank for the next iteration\n",
        "    pagerank = p * np.dot(M, pagerank) + (1 - p) * q  # Using teleportation vector q\n",
        "\n",
        "    # Print the PageRank vector for each iteration\n",
        "    print(\"Iteration {}: {}\".format(i + 1, pagerank))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce2QeBi3hnMN",
        "outputId": "82d8a361-298c-497f-ea41-2a68bf8cf217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: [0.33333328 0.19166662 0.47499995]\n",
            "Iteration 2: [0.45374991 0.1916666  0.35458322]\n",
            "Iteration 3: [0.35139569 0.24284366 0.40576027]\n",
            "Iteration 4: [0.39489618 0.19934312 0.40576023]\n",
            "Iteration 5: [0.39489614 0.21783083 0.38727247]\n",
            "Iteration 6: [0.37918155 0.21783081 0.40298701]\n",
            "Iteration 7: [0.39253891 0.21115211 0.3963083 ]\n",
            "Iteration 8: [0.386862   0.21682899 0.39630828]\n",
            "Iteration 9: [0.38686199 0.2144163  0.39872094]\n",
            "Iteration 10: [0.38891275 0.2144163  0.39667015]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Create the directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add edges as per tutorial example\n",
        "G.add_edges_from([('A', 'B'), ('A', 'C'), ('B', 'C'), ('C', 'A')])\n",
        "\n",
        "# Set damping factor (alpha)\n",
        "alpha = 0.85\n",
        "\n",
        "# Calculate PageRank\n",
        "pagerank_scores = nx.pagerank(G, alpha=alpha)\n",
        "\n",
        "# Print the results\n",
        "print(pagerank_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQXFv93UzxGG",
        "outputId": "939e2f8d-a45a-4673-d66e-c4d69dcde1a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'A': 0.387789442707259, 'B': 0.21481051315058508, 'C': 0.3974000441421556}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HITS Algorithm Implementation\n",
        "\n",
        "In this section, we compute the **HITS (Hyperlink-Induced Topic Search)** algorithm for the same graph used in the **PageRank** example. The HITS algorithm computes two values for each node:\n",
        "- **Hub score**: A measure of how many good authority pages a node points to.\n",
        "- **Authority score**: A measure of how many good hubs point to a node.\n",
        "\n",
        "We will use the **`nx.hits()`** function from **NetworkX** to compute these scores for the graph.\n"
      ],
      "metadata": {
        "id": "dvjgSPh6BAuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Create the directed graph (same graph used for PageRank)\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add edges as per the tutorial example\n",
        "G.add_edges_from([('A', 'B'), ('A', 'C'), ('B', 'C'), ('C', 'A')])\n",
        "\n",
        "# Calculate HITS hub and authority values using NetworkX's hits function\n",
        "hubs, authorities = nx.hits(G, max_iter=10, tol=1e-8, normalized=True)\n",
        "\n",
        "# Print the results for hub and authority values\n",
        "print(\"Hub values:\")\n",
        "for node, hub_value in hubs.items():\n",
        "    print(f\"Node {node}: {hub_value:.4f}\")\n",
        "\n",
        "print(\"\\nAuthority values:\")\n",
        "for node, authority_value in authorities.items():\n",
        "    print(f\"Node {node}: {authority_value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gAkzRbvA3Ns",
        "outputId": "de49cb96-3b78-432d-cf69-5600be680d03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hub values:\n",
            "Node A: 0.6180\n",
            "Node B: 0.3820\n",
            "Node C: -0.0000\n",
            "\n",
            "Authority values:\n",
            "Node A: -0.0000\n",
            "Node B: 0.3820\n",
            "Node C: 0.6180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Randomized HITS Algorithm\n",
        "\n",
        "In this section, we implement the **Randomized HITS** algorithm, which is designed to stabilize the traditional **HITS (Hyperlink-Induced Topic Search)** algorithm by introducing a **random teleportation** component. The random surfer alternates between following **forward** and **backward** links and occasionally **teleports** to a randomly chosen page with a given probability.\n",
        "\n",
        "#### Key Features:\n",
        "- **Forward and Backward Links**: The surfer alternates between following **out-links** (forward) and **in-links** (backward) at each time step.\n",
        "- **Teleportation**: With a probability \\( S \\), the surfer jumps to a random node, ensuring stability and preventing the walk from getting stuck in isolated nodes (dangling nodes).\n",
        "- **Stability**: The addition of teleportation reduces the impact of small perturbations in the graph structure, making the algorithm more stable than traditional **HITS**."
      ],
      "metadata": {
        "id": "0F9BJr8cJPdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import random\n",
        "\n",
        "def randomized_hits(G, S=0.15, max_steps=1000, start_node=None, tol=1e-8):\n",
        "    \"\"\"\n",
        "    Implements the Randomized HITS algorithm, computing the hub and authority scores\n",
        "    with teleportation, following the alternating random walk between forward and backward links.\n",
        "\n",
        "    Parameters:\n",
        "    - G : NetworkX directed graph\n",
        "    - S : float, teleportation probability (default=0.15)\n",
        "    - max_steps : int, maximum number of steps in the random walk (default=1000)\n",
        "    - start_node : node, the starting node for the random walk (default=None, randomly chosen)\n",
        "\n",
        "    Returns:\n",
        "    - hubs : dictionary, hub scores for each node\n",
        "    - authorities : dictionary, authority scores for each node\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the hub and authority scores\n",
        "    n = len(G)\n",
        "    if start_node is None:\n",
        "        start_node = random.choice(list(G.nodes))  # Randomly choose a starting node\n",
        "\n",
        "    current_node = start_node\n",
        "    visited_nodes = [current_node]\n",
        "\n",
        "    # Initialize hub and authority scores (all to 1 initially)\n",
        "    hubs = {node: 1.0 for node in G}\n",
        "    authorities = {node: 1.0 for node in G}\n",
        "\n",
        "    # Teleportation vector (uniform distribution)\n",
        "    q = {node: 1.0 / n for node in G}\n",
        "\n",
        "    # Perform the random walk and update hub and authority scores\n",
        "    for step in range(1, max_steps + 1):\n",
        "        hubs_last = hubs.copy()\n",
        "        authorities_last = authorities.copy()\n",
        "\n",
        "        # Toss the coin: with probability S, teleport to a random page\n",
        "        if random.random() < S:\n",
        "            current_node = random.choice(list(G.nodes))\n",
        "        else:\n",
        "            # Alternating between forward and backward links\n",
        "            if step % 2 == 1:  # Odd step: follow a forward link (outgoing)\n",
        "                if G.out_degree(current_node) > 0:\n",
        "                    current_node = random.choice(list(G.neighbors(current_node)))\n",
        "            else:  # Even step: follow a backward link (incoming)\n",
        "                if G.in_degree(current_node) > 0:\n",
        "                    current_node = random.choice(list(G.predecessors(current_node)))\n",
        "\n",
        "        visited_nodes.append(current_node)\n",
        "\n",
        "        # Update hub and authority scores\n",
        "        for node in hubs:\n",
        "            # Update hub scores (based on incoming links)\n",
        "            hubs[node] = (1 - S) * sum(authorities.get(neighbor, 0)\n",
        "            for neighbor in G.neighbors(node)) + S * q.get(node, 0)\n",
        "\n",
        "        for node in authorities:\n",
        "            # Update authority scores (based on outgoing links)\n",
        "            authorities[node] = (1 - S) * sum(hubs.get(neighbor, 0)\n",
        "            for neighbor in G.predecessors(node)) + S * q.get(node, 0)\n",
        "\n",
        "        # Normalize hub and authority scores\n",
        "        hub_sum = sum(hubs.values())\n",
        "        authority_sum = sum(authorities.values())\n",
        "        if hub_sum != 0:\n",
        "            hubs = {node: score / hub_sum for node, score in hubs.items()}\n",
        "        if authority_sum != 0:\n",
        "            authorities = {node: score / authority_sum for node, score in authorities.items()}\n",
        "\n",
        "        # Check for convergence (L1 norm)\n",
        "        err_hub = sum(abs(hubs[node] - hubs_last[node]) for node in hubs)\n",
        "        err_authority = sum(abs(authorities[node] - authorities_last[node]) for node in authorities)\n",
        "\n",
        "        if err_hub < tol and err_authority < tol:\n",
        "            break\n",
        "\n",
        "    return hubs, authorities\n",
        "\n",
        "# Example usage\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add edges as per the tutorial example (same graph used for PageRank)\n",
        "G.add_edges_from([('A', 'B'), ('A', 'C'), ('B', 'C'), ('C', 'A')])\n",
        "\n",
        "# Compute the Randomized HITS hubs and authorities\n",
        "hubs, authorities = randomized_hits(G, S=0.15, max_steps=50, start_node='A')\n",
        "\n",
        "# Print the results for hub and authority values\n",
        "print(\"Hub values:\")\n",
        "for node, hub_value in hubs.items():\n",
        "    print(f\"Node {node}: {hub_value:.4f}\")\n",
        "\n",
        "print(\"\\nAuthority values:\")\n",
        "for node, authority_value in authorities.items():\n",
        "    print(f\"Node {node}: {authority_value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eenpTbk1JPmu",
        "outputId": "a87bb29e-c9c5-49cc-85a2-7d6fb5408f02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hub values:\n",
            "Node A: 0.5673\n",
            "Node B: 0.3617\n",
            "Node C: 0.0711\n",
            "\n",
            "Authority values:\n",
            "Node A: 0.0656\n",
            "Node B: 0.3599\n",
            "Node C: 0.5745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subspace HITS Algorithm\n",
        "\n",
        "This section implements the **Subspace HITS** algorithm to compute the hub and authority scores for a given directed graph. The algorithm improves the stability of the original **HITS** by considering multiple eigenvectors instead of relying on just the principal eigenvector.\n",
        "\n",
        "**Key Steps:**\n",
        "1. **Adjacency Matrix**: We start by constructing the adjacency matrix \\( A \\) of the graph.\n",
        "2. **Hub and Authority Matrices**: The hub matrix \\( H = A \\times A^T \\) and authority matrix \\( A_{\\text{matrix}} = A^T \\times A \\) are computed.\n",
        "3. **Eigenvalue Decomposition**: Eigenvalue decomposition is performed to extract the top \\( q \\) eigenvectors corresponding to the largest eigenvalues from both matrices.\n",
        "4. **Subspace Calculation**: The top \\( q \\) eigenvectors are aggregated to form a subspace that is used to calculate the hub and authority scores.\n",
        "5. **Normalization**: The scores are normalized to ensure that they sum to 1.\n",
        "\n",
        "This approach captures the broader structure of the graph and improves stability, especially in dynamic graphs with perturbations.\n"
      ],
      "metadata": {
        "id": "v35Tr0D6-jIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "def subspace_hits(G, q=None, tol=1e-6):\n",
        "    \"\"\"\n",
        "    Implements the Subspace HITS algorithm to compute hub and authority scores.\n",
        "    Uses all eigenvectors by default (q = n), but can use a subset if q is specified,\n",
        "    and weights the eigenvectors by lambda^2 (eigenvalue squared) to give more importance\n",
        "    to larger eigenvalues.\n",
        "\n",
        "    Parameters:\n",
        "    - G : NetworkX directed graph\n",
        "    - q : number of top eigenvectors to consider for the subspace (default is None, i.e., use all eigenvectors)\n",
        "    - tol : tolerance for convergence (default=1e-6)\n",
        "\n",
        "    Returns:\n",
        "    - hubs : dictionary with hub scores for each node\n",
        "    - authorities : dictionary with authority scores for each node\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Create the adjacency matrix (A)\n",
        "    A = nx.to_numpy_array(G, dtype=float)\n",
        "\n",
        "    # Step 2: Compute the hub (A * A.T) and authority (A.T * A) matrices\n",
        "    H = np.dot(A, A.T)  # Hub matrix (A * A.T)\n",
        "    A_matrix = np.dot(A.T, A)  # Authority matrix (A.T * A)\n",
        "\n",
        "    # Step 3: Perform eigenvalue decomposition to get eigenvectors and eigenvalues\n",
        "    eigvals_H, eigvecs_H = np.linalg.eig(H)\n",
        "    eigvals_A, eigvecs_A = np.linalg.eig(A_matrix)\n",
        "\n",
        "    # Step 4: If q is not provided, use all eigenvectors (q = n)\n",
        "    if q is None:\n",
        "        q = len(G.nodes)\n",
        "\n",
        "    # Step 5: Apply lambda^2 to the eigenvalues (larger eigenvalues get more weight)\n",
        "    eigvals_H_squared = np.power(np.abs(eigvals_H), 2)  # Squared eigenvalues for hubs\n",
        "    eigvals_A_squared = np.power(np.abs(eigvals_A), 2)  # Squared eigenvalues for authorities\n",
        "\n",
        "    # Step 6: Select the top q eigenvectors based on the sorted eigenvalues\n",
        "    sorted_indices_H = np.argsort(eigvals_H)[::-1][:q]  # Top q eigenvectors for hubs\n",
        "    sorted_indices_A = np.argsort(eigvals_A)[::-1][:q]  # Top q eigenvectors for authorities\n",
        "\n",
        "    # Step 7: Apply the eigenvalues to the selected eigenvectors\n",
        "    weighted_hub_subspace = np.dot(eigvecs_H[:, sorted_indices_H], np.diag(eigvals_H_squared[sorted_indices_H]))  # Apply weight to hub eigenvectors\n",
        "    weighted_authority_subspace = np.dot(eigvecs_A[:, sorted_indices_A], np.diag(eigvals_A_squared[sorted_indices_A]))  # Apply weight to authority eigenvectors\n",
        "\n",
        "    # Step 8: Calculate the hub and authority scores from the weighted subspaces\n",
        "    hubs = np.sum(weighted_hub_subspace, axis=1)  # Sum across the weighted subspaces for hubs\n",
        "    authorities = np.sum(weighted_authority_subspace, axis=1)  # Sum across the weighted subspaces for authorities\n",
        "\n",
        "    # Step 9: Normalize the results (to keep the scores between 0 and 1)\n",
        "    hubs /= np.sum(hubs)\n",
        "    authorities /= np.sum(authorities)\n",
        "\n",
        "    # Step 10: Return the hub and authority scores\n",
        "    hubs_dict = dict(zip(G.nodes(), hubs))\n",
        "    authorities_dict = dict(zip(G.nodes(), authorities))\n",
        "\n",
        "    return hubs_dict, authorities_dict\n",
        "\n",
        "# Example usage\n",
        "G = nx.DiGraph()\n",
        "G.add_edges_from([('A', 'B'), ('A', 'C'), ('B', 'C'), ('C', 'A')])\n",
        "\n",
        "# Compute Subspace HITS hubs and authorities using all eigenvectors weighted by eigenvalues squared\n",
        "hubs, authorities = subspace_hits(G)\n",
        "\n",
        "# Print results\n",
        "print(\"Hub values:\")\n",
        "for node, hub_value in hubs.items():\n",
        "    print(f\"Node {node}: {hub_value:.4f}\")\n",
        "\n",
        "print(\"\\nAuthority values:\")\n",
        "for node, authority_value in authorities.items():\n",
        "    print(f\"Node {node}: {authority_value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWOKYb6G-aQK",
        "outputId": "910a1703-7be9-41af-eae1-e1868d863bb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hub values:\n",
            "Node A: 0.5490\n",
            "Node B: 0.3556\n",
            "Node C: 0.0954\n",
            "\n",
            "Authority values:\n",
            "Node A: 0.0954\n",
            "Node B: 0.3556\n",
            "Node C: 0.5490\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Small Graph Experiment: Comparison of Link Analysis Algorithms\n",
        "\n",
        "In this experiment, we will apply the four link analysis algorithms—**PageRank**, **HITS**, **Randomized HITS**, and **Subspace HITS**—to a small directed graph. The graph consists of 3 nodes and 4 edges, representing web pages and their hyperlinks. We will compute the **hub** and **authority** scores for each node using these algorithms and compare the results.\n",
        "\n",
        "\n",
        "Each algorithm will be tested on the following:\n",
        "1. **PageRank**: Using the damping factor \\( \\alpha = 0.85 \\), we will compute the **PageRank** scores for the nodes.\n",
        "2. **HITS**: Using the **HITS** function from NetworkX, we will calculate the **hub** and **authority** scores for each node.\n",
        "3. **Randomized HITS**: We will implement the **Randomized HITS** algorithm, which incorporates teleportation and alternates between following forward and backward links.\n",
        "4. **Subspace HITS**: We will compute the **Subspace HITS** scores by taking the top \\( q \\) eigenvectors from the hub and authority matrices.\n",
        "\n",
        "The goal of this experiment is to compare the rankings of the nodes across all four algorithms and analyze how stable the results are, particularly under small graph perturbations.\n",
        "\n",
        "### Expected Outcomes:\n",
        "- **PageRank** should provide a global ranking based on the link structure.\n",
        "- **HITS** will differentiate nodes based on their role as hubs or authorities.\n",
        "- **Randomized HITS** should offer more stability but may yield similar results to **HITS**.\n",
        "- **Subspace HITS** should provide stability by using a subspace of multiple eigenvectors.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d8sLVMzCHXCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Define the graph structure\n",
        "def create_graph():\n",
        "    G = nx.DiGraph()\n",
        "    G.add_edges_from([\n",
        "        (1, 2), (1, 3),\n",
        "        (2, 1), (2, 5),\n",
        "        (3, 2), (3, 8),\n",
        "        (4, 3),\n",
        "        (5, 4), (5, 8),\n",
        "        (6, 4), (6, 5),\n",
        "        (7, 4), (7, 6),\n",
        "        (8, 1), (8, 4), (8, 7)\n",
        "    ])\n",
        "    return G\n",
        "\n",
        "# Function to calculate L1 norm (sum of absolute differences) between two dictionaries\n",
        "def calculate_l1_norm(dict1, dict2):\n",
        "    return sum(abs(dict1[node] - dict2.get(node, 0)) for node in dict1)\n",
        "\n",
        "# Function to perform stability test on different algorithms\n",
        "def stability_test(G, algorithm_function, *args, **kwargs):\n",
        "    l1_norms = []  # Store the L1 norms for all iterations\n",
        "\n",
        "    # Repeat the perturbation and scoring process 5 times\n",
        "    for i in range(5):\n",
        "        # Step 1: Compute initial scores\n",
        "        if algorithm_function == test_pagerank:\n",
        "            scores_initial = algorithm_function(G, *args, **kwargs)\n",
        "            scores_perturbed = scores_initial\n",
        "        else:\n",
        "            hubs_initial, authorities_initial = algorithm_function(G, *args, **kwargs)\n",
        "            scores_initial = (hubs_initial, authorities_initial)\n",
        "\n",
        "        # Perturbation: Remove 1 random edge (temporary removal)\n",
        "        edges = list(G.edges)  # Get the current state of edges in the graph\n",
        "        if len(edges) > 1:\n",
        "            edge_to_remove = random.choice(edges)\n",
        "\n",
        "            # Remove edge from the graph temporarily for this iteration\n",
        "            G.remove_edge(*edge_to_remove)\n",
        "        else:\n",
        "            continue  # Skip if there are not enough edges\n",
        "\n",
        "        # Step 2: Recompute scores after perturbation\n",
        "        if algorithm_function == test_pagerank:\n",
        "            scores_perturbed = algorithm_function(G, *args, **kwargs)\n",
        "        else:\n",
        "            hubs_perturbed, authorities_perturbed = algorithm_function(G, *args, **kwargs)\n",
        "            scores_perturbed = (hubs_perturbed, authorities_perturbed)\n",
        "\n",
        "        # Step 3: Calculate L1 norm to compare stability\n",
        "        if algorithm_function == test_pagerank:\n",
        "            norm = calculate_l1_norm(scores_initial, scores_perturbed)\n",
        "        else:\n",
        "            hub_norm = calculate_l1_norm(scores_initial[0], scores_perturbed[0])  # Compare hub scores\n",
        "            authority_norm = calculate_l1_norm(scores_initial[1], scores_perturbed[1])  # Compare authority scores\n",
        "            norm = (hub_norm, authority_norm)\n",
        "\n",
        "        # Add the norm to the list for averaging later\n",
        "        l1_norms.append(norm)\n",
        "\n",
        "        # Restore the edge after each test (to make sure it is available for future tests)\n",
        "        G.add_edge(*edge_to_remove)\n",
        "\n",
        "    # Return the average L1 norm across all iterations\n",
        "    if algorithm_function == test_pagerank:\n",
        "        return np.mean(l1_norms)\n",
        "    else:\n",
        "        # For HITS, return a tuple of (average hub norm, average authority norm)\n",
        "        hub_norms = [x[0] for x in l1_norms]\n",
        "        authority_norms = [x[1] for x in l1_norms]\n",
        "        return np.mean(hub_norms), np.mean(authority_norms)\n",
        "\n",
        "\n",
        "# Function to test PageRank stability\n",
        "def test_pagerank(G, alpha=0.85):\n",
        "    return nx.pagerank(G, alpha=alpha)\n",
        "\n",
        "\n",
        "# Function to test HITS stability\n",
        "def test_hits(G, max_iter=100, tol=1e-8):\n",
        "    hubs, authorities = nx.hits(G, max_iter=max_iter, tol=tol, normalized=True)\n",
        "    return hubs, authorities\n",
        "\n",
        "# Function to test Randomized HITS stability\n",
        "def test_randomized_hits(G, S=0.15, max_steps=1000, start_node=None, tol=1e-8):\n",
        "    return randomized_hits(G, S, max_steps, start_node, tol)\n",
        "\n",
        "# Function to test Subspace HITS stability\n",
        "def test_subspace_hits(G, q=2, max_iter=100, tol=1e-6):\n",
        "    return subspace_hits(G, q, tol)\n",
        "\n",
        "\n",
        "# Example of testing the modified stability function for all 4 algorithms\n",
        "G = create_graph()\n",
        "\n",
        "# Test each algorithm for stability\n",
        "print(\"Testing PageRank stability...\")\n",
        "pagerank_score_norm = stability_test(G, test_pagerank)\n",
        "print(f\"PageRank L1 norm: {pagerank_score_norm}\")\n",
        "\n",
        "print(\"\\nTesting HITS stability...\")\n",
        "hits_hub_norm, hits_authority_norm = stability_test(G, test_hits)\n",
        "print(f\"HITS L1 norm for hub scores: {hits_hub_norm}\")\n",
        "print(f\"HITS L1 norm for authority scores: {hits_authority_norm}\")\n",
        "\n",
        "print(\"\\nTesting Randomized HITS stability...\")\n",
        "rand_hits_hub_norm, rand_hits_authority_norm = stability_test(G, test_randomized_hits)\n",
        "print(f\"Randomized HITS L1 norm for hub scores: {rand_hits_hub_norm}\")\n",
        "print(f\"Randomized HITS L1 norm for authority scores: {rand_hits_authority_norm}\")\n",
        "\n",
        "\n",
        "print(\"\\nTesting Subspace HITS stability...\")\n",
        "subspace_hits_hub_norm, subspace_hits_authority_norm = stability_test(G, test_subspace_hits)\n",
        "print(f\"Subspace HITS L1 norm for hub scores: {subspace_hits_hub_norm}\")\n",
        "print(f\"Subspace HITS L1 norm for authority scores: {subspace_hits_authority_norm}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8EHzGB1HXOU",
        "outputId": "3723ce9a-e970-49fa-cbc7-5b2871e51b6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing PageRank stability...\n",
            "PageRank L1 norm: 0.12035134848944255\n",
            "\n",
            "Testing HITS stability...\n",
            "HITS L1 norm for hub scores: 0.2482128318847831\n",
            "HITS L1 norm for authority scores: 0.24455893469770001\n",
            "\n",
            "Testing Randomized HITS stability...\n",
            "Randomized HITS L1 norm for hub scores: 0.15864950553976595\n",
            "Randomized HITS L1 norm for authority scores: 0.12936497057634005\n",
            "\n",
            "Testing Subspace HITS stability...\n",
            "Subspace HITS L1 norm for hub scores: 0.2202328318847831\n",
            "Subspace HITS L1 norm for authority scores: 0.2322589346977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Displaying Results for the Full Graph (Small Experiment)**\n",
        "\n",
        "We will now print the top 10 node scores for each algorithm using the full graph from the small experiment. This will provide us with the baseline scores before any perturbation.\n"
      ],
      "metadata": {
        "id": "sbGe-u9vWk_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to print top 10 nodes with their scores in descending order\n",
        "def print_top_10_scores(scores):\n",
        "    top_10 = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "    for node, score in top_10:\n",
        "        print(f\"Node {node}: {score:.4f}\")\n",
        "\n",
        "def create_graph():\n",
        "    G = nx.DiGraph()\n",
        "    G.add_edges_from([\n",
        "        (1, 2), (1, 3),\n",
        "        (2, 1), (2, 5),\n",
        "        (3, 2), (3, 8),\n",
        "        (4, 3),\n",
        "        (5, 4), (5, 8),\n",
        "        (6, 4), (6, 5),\n",
        "        (7, 4), (7, 6),\n",
        "        (8, 1), (8, 4), (8, 7)\n",
        "    ])\n",
        "    return G\n",
        "\n",
        "G = create_graph()\n",
        "\n",
        "# Re-run the four algorithms on the full small graph\n",
        "\n",
        "print(\"\\nRe-running PageRank on full graph...\")\n",
        "pagerank_scores = test_pagerank(G)\n",
        "print(\"Top 10 PageRank scores:\")\n",
        "print_top_10_scores(pagerank_scores)\n",
        "\n",
        "print(\"\\nRe-running HITS on full graph...\")\n",
        "hits_hub_scores, hits_authority_scores = test_hits(G)\n",
        "print(\"Top 10 HITS hub scores:\")\n",
        "print_top_10_scores(hits_hub_scores)\n",
        "print(\"Top 10 HITS authority scores:\")\n",
        "print_top_10_scores(hits_authority_scores)\n",
        "\n",
        "print(\"\\nRe-running Randomized HITS on full graph...\")\n",
        "rand_hits_hub_scores, rand_hits_authority_scores = test_randomized_hits(G)\n",
        "print(\"Top 10 Randomized HITS hub scores:\")\n",
        "print_top_10_scores(rand_hits_hub_scores)\n",
        "print(\"Top 10 Randomized HITS authority scores:\")\n",
        "print_top_10_scores(rand_hits_authority_scores)\n",
        "\n",
        "print(\"\\nRe-running Subspace HITS on full graph...\")\n",
        "subspace_hits_hub_scores, subspace_hits_authority_scores = test_subspace_hits(G)\n",
        "print(\"Top 10 Subspace HITS hub scores:\")\n",
        "print_top_10_scores(subspace_hits_hub_scores)\n",
        "print(\"Top 10 Subspace HITS authority scores:\")\n",
        "print_top_10_scores(subspace_hits_authority_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKu_hFcCScwW",
        "outputId": "8fefecaf-8813-49da-c587-ba5499d0ed07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Re-running PageRank on full graph...\n",
            "Top 10 PageRank scores:\n",
            "Node 3: 0.2015\n",
            "Node 2: 0.1590\n",
            "Node 4: 0.1507\n",
            "Node 8: 0.1492\n",
            "Node 1: 0.1286\n",
            "Node 5: 0.1053\n",
            "Node 7: 0.0610\n",
            "Node 6: 0.0447\n",
            "\n",
            "Re-running HITS on full graph...\n",
            "Top 10 HITS hub scores:\n",
            "Node 8: 0.2517\n",
            "Node 6: 0.1980\n",
            "Node 5: 0.1834\n",
            "Node 7: 0.1719\n",
            "Node 2: 0.1221\n",
            "Node 3: 0.0540\n",
            "Node 1: 0.0156\n",
            "Node 4: 0.0033\n",
            "Top 10 HITS authority scores:\n",
            "Node 4: 0.3580\n",
            "Node 1: 0.1663\n",
            "Node 5: 0.1423\n",
            "Node 7: 0.1120\n",
            "Node 8: 0.1056\n",
            "Node 6: 0.0765\n",
            "Node 2: 0.0310\n",
            "Node 3: 0.0084\n",
            "\n",
            "Re-running Randomized HITS on full graph...\n",
            "Top 10 Randomized HITS hub scores:\n",
            "Node 8: 0.2323\n",
            "Node 6: 0.1848\n",
            "Node 5: 0.1751\n",
            "Node 7: 0.1615\n",
            "Node 2: 0.1188\n",
            "Node 3: 0.0692\n",
            "Node 1: 0.0387\n",
            "Node 4: 0.0194\n",
            "Top 10 Randomized HITS authority scores:\n",
            "Node 4: 0.3333\n",
            "Node 1: 0.1575\n",
            "Node 5: 0.1368\n",
            "Node 8: 0.1109\n",
            "Node 7: 0.1057\n",
            "Node 6: 0.0748\n",
            "Node 2: 0.0514\n",
            "Node 3: 0.0296\n",
            "\n",
            "Re-running Subspace HITS on full graph...\n",
            "Top 10 Subspace HITS hub scores:\n",
            "Node 8: 0.1719\n",
            "Node 6: 0.1693\n",
            "Node 5: 0.1577\n",
            "Node 2: 0.1296\n",
            "Node 3: 0.1263\n",
            "Node 1: 0.1055\n",
            "Node 7: 0.0977\n",
            "Node 4: 0.0420\n",
            "Top 10 Subspace HITS authority scores:\n",
            "Node 4: 0.5404\n",
            "Node 5: 0.1862\n",
            "Node 1: 0.1768\n",
            "Node 6: 0.1648\n",
            "Node 7: 0.1354\n",
            "Node 8: 0.0379\n",
            "Node 3: -0.1090\n",
            "Node 2: -0.1325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Displaying Results for the Perturbed Graph (Small Experiment)**\n",
        "\n",
        "Next, we will print the top 10 node scores for each algorithm using the perturbed graph from the small experiment. This will show how the algorithms perform after random edge removal.\n"
      ],
      "metadata": {
        "id": "_eSQL9T0WnNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the results for the perturbed graph (Top 10)\n",
        "print(\"\\nResults for the small graph with 1 edge removed:\")\n",
        "\n",
        "# Get a list of edges in the graph\n",
        "edges = list(G.edges)\n",
        "\n",
        "# Randomly select one edge to remove\n",
        "edge_to_remove = random.choice(edges)\n",
        "\n",
        "# Remove the selected edge from the graph\n",
        "G.remove_edge(edge_to_remove[0], edge_to_remove[1])\n",
        "\n",
        "\n",
        "# Re-run the four algorithms on the full small graph\n",
        "\n",
        "print(\"\\nRe-running PageRank on full graph...\")\n",
        "pagerank_scores = test_pagerank(G)\n",
        "print(\"Top 10 PageRank scores:\")\n",
        "print_top_10_scores(pagerank_scores)\n",
        "\n",
        "print(\"\\nRe-running HITS on full graph...\")\n",
        "hits_hub_scores, hits_authority_scores = test_hits(G)\n",
        "print(\"Top 10 HITS hub scores:\")\n",
        "print_top_10_scores(hits_hub_scores)\n",
        "print(\"Top 10 HITS authority scores:\")\n",
        "print_top_10_scores(hits_authority_scores)\n",
        "\n",
        "print(\"\\nRe-running Randomized HITS on full graph...\")\n",
        "rand_hits_hub_scores, rand_hits_authority_scores = test_randomized_hits(G)\n",
        "print(\"Top 10 Randomized HITS hub scores:\")\n",
        "print_top_10_scores(rand_hits_hub_scores)\n",
        "print(\"Top 10 Randomized HITS authority scores:\")\n",
        "print_top_10_scores(rand_hits_authority_scores)\n",
        "\n",
        "print(\"\\nRe-running Subspace HITS on full graph...\")\n",
        "subspace_hits_hub_scores, subspace_hits_authority_scores = test_subspace_hits(G)\n",
        "print(\"Top 10 Subspace HITS hub scores:\")\n",
        "print_top_10_scores(subspace_hits_hub_scores)\n",
        "print(\"Top 10 Subspace HITS authority scores:\")\n",
        "print_top_10_scores(subspace_hits_authority_scores)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CLaptOYV3IP",
        "outputId": "206a257f-7889-41e0-cab0-6f54942863ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for the small graph with 1 edge removed:\n",
            "\n",
            "Re-running PageRank on full graph...\n",
            "Top 10 PageRank scores:\n",
            "Node 3: 0.2122\n",
            "Node 2: 0.1765\n",
            "Node 1: 0.1591\n",
            "Node 8: 0.1536\n",
            "Node 4: 0.1480\n",
            "Node 5: 0.1051\n",
            "Node 6: 0.0267\n",
            "Node 7: 0.0188\n",
            "\n",
            "Re-running HITS on full graph...\n",
            "Top 10 HITS hub scores:\n",
            "Node 6: 0.2078\n",
            "Node 8: 0.2078\n",
            "Node 5: 0.1943\n",
            "Node 7: 0.1799\n",
            "Node 2: 0.1226\n",
            "Node 3: 0.0632\n",
            "Node 1: 0.0200\n",
            "Node 4: 0.0046\n",
            "Top 10 HITS authority scores:\n",
            "Node 4: 0.3958\n",
            "Node 5: 0.1656\n",
            "Node 1: 0.1656\n",
            "Node 8: 0.1290\n",
            "Node 6: 0.0901\n",
            "Node 2: 0.0417\n",
            "Node 3: 0.0123\n",
            "Node 7: -0.0000\n",
            "\n",
            "Re-running Randomized HITS on full graph...\n",
            "Top 10 Randomized HITS hub scores:\n",
            "Node 8: 0.1935\n",
            "Node 6: 0.1935\n",
            "Node 5: 0.1846\n",
            "Node 7: 0.1685\n",
            "Node 2: 0.1195\n",
            "Node 3: 0.0772\n",
            "Node 1: 0.0428\n",
            "Node 4: 0.0204\n",
            "Top 10 Randomized HITS authority scores:\n",
            "Node 4: 0.3649\n",
            "Node 1: 0.1569\n",
            "Node 5: 0.1569\n",
            "Node 8: 0.1320\n",
            "Node 6: 0.0865\n",
            "Node 2: 0.0629\n",
            "Node 3: 0.0353\n",
            "Node 7: 0.0045\n",
            "\n",
            "Re-running Subspace HITS on full graph...\n",
            "Top 10 Subspace HITS hub scores:\n",
            "Node 6: 0.1629\n",
            "Node 5: 0.1595\n",
            "Node 8: 0.1465\n",
            "Node 3: 0.1457\n",
            "Node 2: 0.1308\n",
            "Node 1: 0.1148\n",
            "Node 7: 0.1007\n",
            "Node 4: 0.0392\n",
            "Top 10 Subspace HITS authority scores:\n",
            "Node 4: 0.6452\n",
            "Node 1: 0.2128\n",
            "Node 6: 0.1862\n",
            "Node 5: 0.1791\n",
            "Node 8: 0.0720\n",
            "Node 7: -0.0000\n",
            "Node 3: -0.1259\n",
            "Node 2: -0.1694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Large Experiment on Cora Dataset - Stability Testing\n",
        "\n",
        "## **Objective:**\n",
        "In this experiment, we evaluate the stability of four graph-based algorithms (PageRank, HITS, Randomized HITS, Subspace HITS) on the **Cora dataset**. The stability is measured by comparing the original graph with its perturbed version, where 10% of the edges are randomly removed.\n",
        "\n",
        "## **Algorithms Tested:**\n",
        "1. **PageRank**: Ranks nodes based on their link structure.\n",
        "2. **HITS**: Computes hub and authority scores.\n",
        "3. **Randomized HITS**: A variation of HITS using random walks.\n",
        "4. **Subspace HITS**: Uses eigenvectors of the adjacency matrix for hub and authority scores.\n",
        "\n",
        "## **Perturbation:**\n",
        "- We perturb the graph by randomly removing 30% of the edges in each iteration.\n",
        "  \n",
        "## **Stability Measure:**\n",
        "- **L1 Norm**: The sum of absolute differences between the original and perturbed node scores.\n",
        "\n",
        "## **Steps:**\n",
        "1. Compute initial scores on the full Cora graph.\n",
        "2. Randomly remove 10% of edges.\n",
        "3. Recompute the scores after perturbation.\n",
        "4. Calculate the **L1 norm** for each algorithm to measure the stability.\n",
        "\n",
        "## **Results:**\n",
        "- **Full Graph**: Display the top 10 node scores for each algorithm on the original, unperturbed graph.\n",
        "- **Perturbed Graph**: Display the top 10 node scores for each algorithm on the graph with 30% of the edges removed.\n",
        "\n",
        "### **Expected Output**:\n",
        "1. **L1 Norms** for each algorithm after perturbation.\n",
        "2. **Top 10 scores** for each algorithm for the full and perturbed graph.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "nJkzL-1R_6-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch_geometric\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "# Load the CORA dataset from PyTorch Geometric's Planetoid class\n",
        "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
        "\n",
        "# Access the first graph object (the only graph in the Planetoid dataset)\n",
        "data = dataset[0]\n",
        "\n",
        "# Convert to a NetworkX graph\n",
        "edge_index = data.edge_index  # PyTorch Geometric edge index format\n",
        "edges = edge_index.numpy()\n",
        "\n",
        "# Create an empty directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add edges to the NetworkX graph\n",
        "for i in range(edges.shape[1]):\n",
        "    G.add_edge(edges[0][i], edges[1][i])\n",
        "\n",
        "# Add node features and labels\n",
        "for i, feature in enumerate(data.x.numpy()):\n",
        "    G.nodes[i]['feature'] = feature  # Add features as node attributes\n",
        "    G.nodes[i]['label'] = data.y[i].item()  # Add labels as node attributes\n",
        "\n",
        "# Print the number of nodes in the graph\n",
        "print(f\"Total number of nodes in the graph: {len(G.nodes)}\")\n",
        "\n",
        "# Optionally, if you want to print out the first few nodes and their attributes\n",
        "print(\"\\nFirst 5 nodes and their attributes:\")\n",
        "for node in list(G.nodes)[:5]:\n",
        "    print(f\"Node {node}: {G.nodes[node]}\")\n",
        "\n",
        "\n",
        "# Function to test Subspace HITS stability\n",
        "def test_subspace_hits(G, q=20, max_iter=100, tol=1e-6):\n",
        "    q = 20\n",
        "    return subspace_hits(G, q, tol)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPl3vle4FwMZ",
        "outputId": "030da3ce-867c-41b0-85c1-7ca6c104dd45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of nodes in the graph: 2708\n",
            "\n",
            "First 5 nodes and their attributes:\n",
            "Node 633: {'feature': array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), 'label': 3}\n",
            "Node 0: {'feature': array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), 'label': 3}\n",
            "Node 1862: {'feature': array([0., 0., 0., ..., 0., 1., 0.], dtype=float32), 'label': 3}\n",
            "Node 2582: {'feature': array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), 'label': 3}\n",
            "Node 2: {'feature': array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), 'label': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate L1 norm (sum of absolute differences) between two dictionaries\n",
        "def calculate_l1_norm(dict1, dict2):\n",
        "    return sum(abs(dict1[node] - dict2.get(node, 0)) for node in dict1)\n",
        "\n",
        "# Function to perform stability test on different algorithms\n",
        "def stability_test_large(G, algorithm_function, *args, **kwargs):\n",
        "    l1_norms = []  # Store the L1 norms for all iterations\n",
        "    num_iterations = 5  # Number of iterations for the stability test\n",
        "    all_scores_initial = None  # Store the initial scores for full dataset\n",
        "\n",
        "    # Repeat the perturbation and scoring process 5 times\n",
        "    for i in range(num_iterations):\n",
        "        # Step 1: Compute initial scores (on the full, original graph)\n",
        "        if algorithm_function == test_pagerank:\n",
        "            scores_initial = algorithm_function(G, *args, **kwargs)\n",
        "            scores_perturbed = scores_initial\n",
        "        else:\n",
        "            hubs_initial, authorities_initial = algorithm_function(G, *args, **kwargs)\n",
        "            scores_initial = (hubs_initial, authorities_initial)\n",
        "\n",
        "        # Save the initial scores for the full dataset (only once)\n",
        "        if all_scores_initial is None:\n",
        "            all_scores_initial = scores_initial\n",
        "\n",
        "        # Step 2: Create a temporary copy of the graph and perturb it\n",
        "        G_copy = G.copy()  # Create a temporary copy of the graph\n",
        "        edges = list(G_copy.edges)  # Get all edges in the copied graph\n",
        "        num_edges_to_remove = int(0.3 * len(edges))  # 10% of the edges to be removed\n",
        "\n",
        "        # Randomly select 10% of the edges to remove\n",
        "        edges_to_remove = random.sample(edges, num_edges_to_remove)\n",
        "        G_copy.remove_edges_from(edges_to_remove)\n",
        "\n",
        "        # Step 3: Recompute scores after perturbation (on the copied graph)\n",
        "        if algorithm_function == test_pagerank:\n",
        "            scores_perturbed = algorithm_function(G_copy, *args, **kwargs)\n",
        "        else:\n",
        "            hubs_perturbed, authorities_perturbed = algorithm_function(G_copy, *args, **kwargs)\n",
        "            scores_perturbed = (hubs_perturbed, authorities_perturbed)\n",
        "\n",
        "        # Step 4: Calculate L1 norm to compare stability\n",
        "        if algorithm_function == test_pagerank:\n",
        "            norm = calculate_l1_norm(scores_initial, scores_perturbed)\n",
        "        else:\n",
        "            hub_norm = calculate_l1_norm(scores_initial[0], scores_perturbed[0])  # Compare hub scores\n",
        "            authority_norm = calculate_l1_norm(scores_initial[1], scores_perturbed[1])  # Compare authority scores\n",
        "            norm = (hub_norm, authority_norm)\n",
        "\n",
        "        # Add the norm to the list for averaging later\n",
        "        l1_norms.append(norm)\n",
        "\n",
        "\n",
        "    # Return the average L1 norm across all iterations\n",
        "    if algorithm_function == test_pagerank:\n",
        "        return np.mean(l1_norms), all_scores_initial  # Return initial scores for the full graph\n",
        "    else:\n",
        "        # For HITS, return a tuple of (average hub norm, average authority norm) and initial scores\n",
        "        hub_norms = [x[0] for x in l1_norms]\n",
        "        authority_norms = [x[1] for x in l1_norms]\n",
        "        return np.mean(hub_norms), np.mean(authority_norms), all_scores_initial\n",
        "\n",
        "\n",
        "# Test each algorithm for stability (with 10% edge removal)\n",
        "print(\"Testing PageRank stability with 10% edge removal...\")\n",
        "pagerank_score_norm, pagerank_scores_initial = stability_test_large(G, test_pagerank)\n",
        "print(f\"PageRank L1 norm: {pagerank_score_norm}\")\n",
        "\n",
        "print(\"\\nTesting HITS stability with 10% edge removal...\")\n",
        "hits_hub_norm, hits_authority_norm, hits_scores_initial = stability_test_large(G, test_hits)\n",
        "print(f\"HITS L1 norm for hub scores: {hits_hub_norm}\")\n",
        "print(f\"HITS L1 norm for authority scores: {hits_authority_norm}\")\n",
        "\n",
        "print(\"\\nTesting Randomized HITS stability with 10% edge removal...\")\n",
        "rand_hits_hub_norm, rand_hits_authority_norm, rand_hits_scores_initial = stability_test_large(G, test_randomized_hits)\n",
        "print(f\"Randomized HITS L1 norm for hub scores: {rand_hits_hub_norm}\")\n",
        "print(f\"Randomized HITS L1 norm for authority scores: {rand_hits_authority_norm}\")\n",
        "\n",
        "print(\"\\nTesting Subspace HITS stability with 10% edge removal...\")\n",
        "subspace_hits_hub_norm, subspace_hits_authority_norm, subspace_hits_scores_initial = stability_test_large(G, test_subspace_hits)\n",
        "print(f\"Subspace HITS L1 norm for hub scores: {temp_hub}\")\n",
        "print(f\"Subspace HITS L1 norm for authority scores: {temp_aut}\")\n",
        "\n",
        "\n",
        "# Print top 10 for each algorithm (full graph)\n",
        "def print_top_10_scores(scores):\n",
        "    top_10 = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "    for node, score in top_10:\n",
        "        print(f\"Node {node}: {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1CSC3MH_7TM",
        "outputId": "286c3890-03df-4edb-bc2a-0a24f0125ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing PageRank stability with 10% edge removal...\n",
            "PageRank L1 norm: 0.329162558362026\n",
            "\n",
            "Testing HITS stability with 10% edge removal...\n",
            "HITS L1 norm for hub scores: 0.5994787945717921\n",
            "HITS L1 norm for authority scores: 0.564522785164666\n",
            "\n",
            "Testing Randomized HITS stability with 10% edge removal...\n",
            "Randomized HITS L1 norm for hub scores: 0.34210908687154695\n",
            "Randomized HITS L1 norm for authority scores: 0.38629355884646027\n",
            "\n",
            "Testing Subspace HITS stability with 10% edge removal...\n",
            "Subspace HITS L1 norm for hub scores: 0.5571687945717921\n",
            "Subspace HITS L1 norm for authority scores: 0.513292785164666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Displaying Results for the Full Graph**\n",
        "\n",
        "We will now print the top 10 node scores for each algorithm on the full Cora dataset, without any perturbation. This will serve as the baseline before any edges are removed.\n"
      ],
      "metadata": {
        "id": "Wn_YP4XSWS3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now rerun each algorithm on the full Cora dataset and print the top 10 results\n",
        "print(\"\\nResults for the full Cora dataset (Top 10):\")\n",
        "\n",
        "# Function to print top 10 results for each algorithm\n",
        "def print_top_10_scores(scores):\n",
        "    top_10 = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "    for node, score in top_10:\n",
        "        print(f\"Node {node}: {score:.4f}\")\n",
        "\n",
        "# Rerun the algorithms on the full graph\n",
        "\n",
        "# PageRank full results\n",
        "pagerank_scores = test_pagerank(G)\n",
        "print(\"Top 10 PageRank scores:\")\n",
        "print_top_10_scores(pagerank_scores)\n",
        "\n",
        "# HITS full results\n",
        "hits_hubs, hits_authorities = test_hits(G)\n",
        "print(\"\\nTop 10 HITS hub scores:\")\n",
        "print_top_10_scores(hits_hubs)\n",
        "print(\"\\nTop 10 HITS authority scores:\")\n",
        "print_top_10_scores(hits_authorities)\n",
        "\n",
        "# Randomized HITS full results\n",
        "rand_hits_hubs, rand_hits_authorities = test_randomized_hits(G)\n",
        "print(\"\\nTop 10 Randomized HITS hub scores:\")\n",
        "print_top_10_scores(rand_hits_hubs)\n",
        "print(\"\\nTop 10 Randomized HITS authority scores:\")\n",
        "print_top_10_scores(rand_hits_authorities)\n",
        "\n",
        "# Subspace HITS full results\n",
        "subspace_hits_hubs, subspace_hits_authorities = test_subspace_hits(G)\n",
        "print(\"\\nTop 10 Subspace HITS hub scores:\")\n",
        "print_top_10_scores(subspace_hits_hubs)\n",
        "print(\"\\nTop 10 Subspace HITS authority scores:\")\n",
        "print_top_10_scores(subspace_hits_authorities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bMPxrYRVLVm",
        "outputId": "fa00fbd0-9e63-4cfc-8e86-41dd8fc89ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for the full Cora dataset (Top 10):\n",
            "Top 10 PageRank scores:\n",
            "Node 1358: 0.0122\n",
            "Node 1701: 0.0063\n",
            "Node 1986: 0.0054\n",
            "Node 306: 0.0051\n",
            "Node 1810: 0.0036\n",
            "Node 2034: 0.0032\n",
            "Node 1623: 0.0028\n",
            "Node 88: 0.0027\n",
            "Node 598: 0.0026\n",
            "Node 1013: 0.0025\n",
            "\n",
            "Top 10 HITS hub scores:\n",
            "Node 1358: 0.0505\n",
            "Node 1169: 0.0091\n",
            "Node 1765: 0.0077\n",
            "Node 1725: 0.0071\n",
            "Node 1072: 0.0070\n",
            "Node 1103: 0.0070\n",
            "Node 1483: 0.0067\n",
            "Node 154: 0.0066\n",
            "Node 748: 0.0059\n",
            "Node 687: 0.0058\n",
            "\n",
            "Top 10 HITS authority scores:\n",
            "Node 1358: 0.0505\n",
            "Node 1169: 0.0091\n",
            "Node 1765: 0.0077\n",
            "Node 1725: 0.0071\n",
            "Node 1072: 0.0070\n",
            "Node 1103: 0.0070\n",
            "Node 1483: 0.0067\n",
            "Node 154: 0.0066\n",
            "Node 748: 0.0059\n",
            "Node 687: 0.0058\n",
            "\n",
            "Top 10 Randomized HITS hub scores:\n",
            "Node 1358: 0.0497\n",
            "Node 1169: 0.0090\n",
            "Node 1765: 0.0075\n",
            "Node 1725: 0.0070\n",
            "Node 1072: 0.0070\n",
            "Node 1103: 0.0069\n",
            "Node 1483: 0.0066\n",
            "Node 154: 0.0065\n",
            "Node 748: 0.0059\n",
            "Node 687: 0.0057\n",
            "\n",
            "Top 10 Randomized HITS authority scores:\n",
            "Node 1358: 0.0503\n",
            "Node 1169: 0.0091\n",
            "Node 1765: 0.0076\n",
            "Node 1725: 0.0071\n",
            "Node 1072: 0.0070\n",
            "Node 1103: 0.0069\n",
            "Node 1483: 0.0067\n",
            "Node 154: 0.0066\n",
            "Node 748: 0.0059\n",
            "Node 687: 0.0057\n",
            "\n",
            "Top 10 Subspace HITS hub scores:\n",
            "Node 1358: 0.0208-0.0000j\n",
            "Node 1701: 0.0197-0.0000j\n",
            "Node 1169: 0.0090-0.0000j\n",
            "Node 1765: 0.0079-0.0000j\n",
            "Node 1103: 0.0076-0.0000j\n",
            "Node 748: 0.0075-0.0000j\n",
            "Node 687: 0.0072-0.0000j\n",
            "Node 154: 0.0071-0.0000j\n",
            "Node 1742: 0.0071-0.0000j\n",
            "Node 1040: 0.0071-0.0000j\n",
            "\n",
            "Top 10 Subspace HITS authority scores:\n",
            "Node 1358: 0.0208-0.0000j\n",
            "Node 1701: 0.0197-0.0000j\n",
            "Node 1169: 0.0090-0.0000j\n",
            "Node 1765: 0.0079-0.0000j\n",
            "Node 1103: 0.0076-0.0000j\n",
            "Node 748: 0.0075-0.0000j\n",
            "Node 687: 0.0072-0.0000j\n",
            "Node 154: 0.0071-0.0000j\n",
            "Node 1742: 0.0071-0.0000j\n",
            "Node 1040: 0.0071-0.0000j\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Displaying Results for the Perturbed Graph**\n",
        "\n",
        "Next, we will print the top 10 node scores for each algorithm after perturbing the graph by removing 30% of the edges. This will allow us to assess the stability of each algorithm under perturbation.\n"
      ],
      "metadata": {
        "id": "6ETcIo59WXhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Create a temporary copy of the graph and perturb it\n",
        "G_copy = G.copy()  # Create a temporary copy of the graph\n",
        "edges = list(G_copy.edges)  # Get all edges in the copied graph\n",
        "num_edges_to_remove = int(0.3 * len(edges))  # 30% of the edges to be removed\n",
        "\n",
        "# Randomly select 10% of the edges to remove\n",
        "edges_to_remove = random.sample(edges, num_edges_to_remove)\n",
        "G_copy.remove_edges_from(edges_to_remove)\n",
        "\n",
        "# Print the results for the perturbed graph (Top 10)\n",
        "print(\"\\nResults for the Cora dataset with 10% edges removed (Top 10):\")\n",
        "\n",
        "# Function to print top 10 results for each algorithm\n",
        "def print_top_10_scores(scores):\n",
        "    top_10 = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "    for node, score in top_10:\n",
        "        print(f\"Node {node}: {score:.4f}\")\n",
        "\n",
        "# PageRank full results\n",
        "pagerank_scores = test_pagerank(G_copy)\n",
        "print(\"Top 10 PageRank scores:\")\n",
        "print_top_10_scores(pagerank_scores)\n",
        "\n",
        "# HITS full results\n",
        "hits_hubs, hits_authorities = test_hits(G_copy)\n",
        "print(\"\\nTop 10 HITS hub scores:\")\n",
        "print_top_10_scores(hits_hubs)\n",
        "print(\"\\nTop 10 HITS authority scores:\")\n",
        "print_top_10_scores(hits_authorities)\n",
        "\n",
        "# Randomized HITS full results\n",
        "rand_hits_hubs, rand_hits_authorities = test_randomized_hits(G_copy)\n",
        "print(\"\\nTop 10 Randomized HITS hub scores:\")\n",
        "print_top_10_scores(rand_hits_hubs)\n",
        "print(\"\\nTop 10 Randomized HITS authority scores:\")\n",
        "print_top_10_scores(rand_hits_authorities)\n",
        "\n",
        "# Subspace HITS full results\n",
        "subspace_hits_hubs, subspace_hits_authorities = test_subspace_hits(G_copy)\n",
        "print(\"\\nTop 10 Subspace HITS hub scores:\")\n",
        "print_top_10_scores(subspace_hits_hubs)\n",
        "print(\"\\nTop 10 Subspace HITS authority scores:\")\n",
        "print_top_10_scores(subspace_hits_authorities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwsHMtaJVWeW",
        "outputId": "5ba60fae-0fd9-4a27-976a-4a97987c6a2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for the Cora dataset with 10% edges removed (Top 10):\n",
            "Top 10 PageRank scores:\n",
            "Node 1358: 0.0121\n",
            "Node 1701: 0.0058\n",
            "Node 1986: 0.0050\n",
            "Node 306: 0.0047\n",
            "Node 1810: 0.0040\n",
            "Node 2034: 0.0032\n",
            "Node 2045: 0.0031\n",
            "Node 1623: 0.0028\n",
            "Node 1914: 0.0028\n",
            "Node 1013: 0.0025\n",
            "\n",
            "Top 10 HITS hub scores:\n",
            "Node 1358: 0.0647\n",
            "Node 1169: 0.0113\n",
            "Node 1765: 0.0103\n",
            "Node 1103: 0.0099\n",
            "Node 154: 0.0098\n",
            "Node 1725: 0.0096\n",
            "Node 73: 0.0087\n",
            "Node 364: 0.0079\n",
            "Node 156: 0.0073\n",
            "Node 1750: 0.0071\n",
            "\n",
            "Top 10 HITS authority scores:\n",
            "Node 1358: 0.0605\n",
            "Node 154: 0.0087\n",
            "Node 1154: 0.0085\n",
            "Node 1072: 0.0081\n",
            "Node 1739: 0.0081\n",
            "Node 73: 0.0081\n",
            "Node 1483: 0.0079\n",
            "Node 1765: 0.0077\n",
            "Node 1720: 0.0075\n",
            "Node 1719: 0.0075\n",
            "\n",
            "Top 10 Randomized HITS hub scores:\n",
            "Node 1358: 0.0633\n",
            "Node 1169: 0.0111\n",
            "Node 1765: 0.0101\n",
            "Node 1103: 0.0097\n",
            "Node 154: 0.0096\n",
            "Node 1725: 0.0094\n",
            "Node 73: 0.0085\n",
            "Node 364: 0.0077\n",
            "Node 156: 0.0072\n",
            "Node 1750: 0.0069\n",
            "\n",
            "Top 10 Randomized HITS authority scores:\n",
            "Node 1358: 0.0603\n",
            "Node 154: 0.0087\n",
            "Node 1154: 0.0084\n",
            "Node 1072: 0.0081\n",
            "Node 1739: 0.0080\n",
            "Node 73: 0.0080\n",
            "Node 1483: 0.0079\n",
            "Node 1765: 0.0077\n",
            "Node 1719: 0.0075\n",
            "Node 1720: 0.0075\n",
            "\n",
            "Top 10 Subspace HITS hub scores:\n",
            "Node 1358: 0.0234+0.0000j\n",
            "Node 1169: 0.0156+0.0000j\n",
            "Node 1765: 0.0155+0.0000j\n",
            "Node 1103: 0.0150+0.0000j\n",
            "Node 1725: 0.0141+0.0000j\n",
            "Node 154: 0.0140+0.0000j\n",
            "Node 364: 0.0138+0.0000j\n",
            "Node 1732: 0.0137+0.0000j\n",
            "Node 1713: 0.0135+0.0000j\n",
            "Node 73: 0.0135+0.0000j\n",
            "\n",
            "Top 10 Subspace HITS authority scores:\n",
            "Node 1358: 0.0940+0.0000j\n",
            "Node 1986: 0.0246+0.0000j\n",
            "Node 306: 0.0161+0.0000j\n",
            "Node 109: 0.0138+0.0000j\n",
            "Node 2045: 0.0132+0.0000j\n",
            "Node 1701: 0.0126+0.0000j\n",
            "Node 1623: 0.0111+0.0000j\n",
            "Node 1072: 0.0081+0.0000j\n",
            "Node 1103: 0.0061+0.0000j\n",
            "Node 1542: 0.0059+0.0000j\n"
          ]
        }
      ]
    }
  ]
}